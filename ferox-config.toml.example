# Professional Bug Hunter Configuration for Feroxbuster
# Optimized for bug bounty hunting and penetration testing

# =============================================================================
# CORE SETTINGS
# =============================================================================

# Wordlist - Comprehensive directory list for thorough scanning
# wordlist = "/wordlists/seclists/Discovery/Web-Content/raft-medium-directories.txt"

# Output format - JSON for easy parsing and automation
# json = true
# output = "feroxbuster_results.txt"

# =============================================================================
# STATUS CODES - Report these HTTP codes
# =============================================================================

# Comprehensive list for bug hunting (success, redirects, auth errors, server errors)
status_codes = [200, 201, 204, 401, 403, 405, 500]

# Filter out 404 Not Found to reduce noise
filter_status = [404, 429, 503]

# Replay interesting responses through proxy (optional)
# replay_codes = [200, 401, 403, 500]

# =============================================================================
# PERFORMANCE & STEALTH SETTINGS
# =============================================================================

# Thread count - balance between speed and stealth
threads = 25

# Parallel scans
parallel = 4

# Scan limit - max concurrent directory scans
scan_limit = 4

# Rate limiting - requests per second (polite scanning)
rate_limit = 70

# Timeout per request in seconds
timeout = 10

# Auto-tune based on target response time
auto_tune = true

# Auto-bail if target becomes unresponsive
auto_bail = true

# =============================================================================
# USER AGENT CONFIGURATION
# =============================================================================

# Rotate user agents to avoid detection
random_agent = true

# Fallback user agent if random_agent is false
user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0"

# =============================================================================
# EXTENSIONS - Files to scan for
# =============================================================================

# Comprehensive extension list for bug bounty hunting
extensions = [
    # Web technologies
    "php", "php3", "php4", "php5", "phtml", "inc",
    "html", "htm", "shtml", "dhtml",
    "js", "json", "xml", "yaml", "yml", "toml",
    "asp", "aspx", "ashx", "asmx", "axd",
    "jsp", "jspx", "jsw", "jsv", "do", "action",
    "cgi", "pl", "py", "rb", "sh",

    # Config files
    "config", "conf", "cfg", "ini", "env", "properties",
    "dist", "local", "sample", "example", "template",

    # Backup files
    "bak", "backup", "old", "orig", "original", "save",
    "swp", "tmp", "temp", "copy", "~",

    # Archives
    "zip", "tar", "gz", "bz2", "7z", "rar", "tgz",

    # Databases
    "sql", "db", "sqlite", "sqlite3", "mdb",

    # Documents
    "txt", "md", "pdf", "doc", "docx", "xls", "xlsx", "csv",

    # Logs
    "log", "logs",

    # Version control
    "git", "svn", "hg",

    # Security/Certs
    "pem", "crt", "cer", "key"
]

# Automatically discover backup files
collect_backups = true

# Automatically discover extensions
collect_extensions = true

# Skip these file types to reduce noise
dont_collect = ["jpg","jpeg","png","gif","ico","svg","css","js.map","woff","woff2","ttf","eot"]

# =============================================================================
# RECURSION & DEPTH
# =============================================================================

# Recursion depth
depth = 4

# Force recursion into all directories
force_recursion = false

# Extract links from HTML/JS responses
extract_links = false

# Add trailing slash to directories
add_slash = true

# =============================================================================
# FILTERING - Remove false positives
# =============================================================================

# Filter by response size (add your target's 404 page size here)
filter_size = [216, 273, 556, 1234, 1337, 16000]

# Filter by word count
filter_word_count = [0, 1, 2, 3, 4, 5]

# Filter similar pages (anti-soft-404)
filter_similar = true

# Filter by regex pattern
filter_regex = [
  "404 Not Found",
  "Page Not Found",
  "page you requested",
  "does not exist",
  "not found on this server"
]


# =============================================================================
# HTTP METHODS
# =============================================================================

# HTTP methods to test
methods = ["GET"]

# =============================================================================
# REDIRECTS & SSL
# =============================================================================

# Follow redirects
redirects = true

# Allow insecure SSL certificates (useful for staging/dev environments)
insecure = true

# Regex denylist
# regex_denylist = ["/deny.*"]

# =============================================================================
# OUTPUT & LOGGING
# =============================================================================

# Verbosity: 0=silent, 1=normal, 2=verbose
verbosity = 1

# Quiet mode - only show results
quiet = false

# Silent mode - no output at all
silent = false

# Debug log for troubleshooting
debug_log = "feroxbuster_debug.log"

# Save state for resuming scans
save_state = true

# Time limit (0 = unlimited)
time_limit = "0"

# Scan directory listings
scan_dir_listings = true

# Show unique responses only
unique = true

# Response size limit (4MB)
response_size_limit = 4194304

# Protocol
protocol = "https"

# =============================================================================
# PROXY CONFIGURATION (Optional)
# =============================================================================

# HTTP proxy (e.g., Burp Suite)
# proxy = "http://127.0.0.1:8080"

# Replay proxy for interesting responses
# replay_proxy = "http://127.0.0.1:8080"

# Replay codes - which responses to send to replay proxy
# replay_codes = [200, 302, 401, 403, 500]

# =============================================================================
# ADVANCED SETTINGS
# =============================================================================

# Collect words from responses for wordlist building
collect_words = false

# Query parameter fuzzing
# queries = [["page", "FUZZ"], ["id", "FUZZ"]]

# Read URLs from stdin
stdin = false

# Don't filter responses (show everything)
# dont_filter = true

# Limit number of progress bars
# limit_bars = 3

# Disable recursion
# no_recursion = true

# =============================================================================
# CUSTOM HEADERS
# =============================================================================

[headers]
Accept = "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
Accept-Language = "en-US,en;q=0.9"
Accept-Encoding = "gzip, deflate, br"
Connection = "keep-alive"
Upgrade-Insecure-Requests = "1"
Sec-Fetch-Dest = "document"
Sec-Fetch-Mode = "navigate"
Sec-Fetch-Site = "none"
DNT = "1"
